import streamlit as st
import google.generativeai as genai
import os
import re
import time # NOVO: Precisamos da biblioteca 'time' para o 'sleep'

st.set_page_config(layout="wide")
st.title("üß™ Analisador de Ideias v1.1.4")

@st.cache_resource
def load_models():
    try:
        api_key = os.environ["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
    except KeyError:
        st.error("Erro: A vari√°vel de ambiente GOOGLE_API_KEY n√£o foi encontrada.")
        return None, None
    
    generation_config = {"temperature": 0.5, "max_output_tokens": 8192}
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    
    model_pro = genai.GenerativeModel("models/gemini-2.5-pro", generation_config=generation_config, safety_settings=safety_settings)
    model_flash = genai.GenerativeModel("models/gemini-2.5-flash", generation_config=generation_config, safety_settings=safety_settings)
    return model_pro, model_flash

model_pro, model_flash = load_models()
if not model_pro or not model_flash:
    st.stop()

# Parser Robusto (sem mudan√ßas)
def extrair_bloco_robusto(texto_completo, bloco_atual, proximo_bloco=None):
    if texto_completo is None: return None
    start_tag = f"[---{bloco_atual}_START---]"
    if proximo_bloco:
        end_tag_pattern = f"[---{proximo_bloco}_START---]"
    else:
        end_tag_pattern = f"[---{bloco_atual}_END---]"
    try:
        start_index = texto_completo.index(start_tag) + len(start_tag)
        captured_text = ""
        try:
            end_index = texto_completo.index(end_tag_pattern, start_index)
            captured_text = texto_completo[start_index:end_index]
        except ValueError:
            captured_text = texto_completo[start_index:]
        cleaned_text = re.sub(r'\[---.*?_END---\]', '', captured_text, flags=re.DOTALL)
        return cleaned_text.strip()
    except ValueError:
        return None

# NOVO: Fun√ß√£o "segura" com Backoff Exponencial
def safe_generate_content(prompt, model, max_retries=3):
    """
    Tenta gerar conte√∫do com o modelo, aplicando backoff exponencial
    em caso de erro de limite de taxa (429).
    Retorna o response.text em sucesso, ou None em falha.
    """
    wait_time = 1 # Come√ßa esperando 1 segundo
    for i in range(max_retries):
        try:
            # Tenta a chamada de API
            response = model.generate_content(prompt)
            return response.text # Sucesso!
        except Exception as e:
            # Verifica se √© um erro de limite de taxa
            # (A API do Google joga um erro gen√©rico, mas o texto cont√©m '429')
            if "429" in str(e):
                st.warning(f"Limite de taxa atingido para {model.model_name} (Tentativa {i+1}/{max_retries}). Esperando {wait_time}s...")
                time.sleep(wait_time)
                wait_time *= 2 # Backoff exponencial: 1s, 2s, 4s
            else:
                # Foi outro erro (ex: seguran√ßa, 500)
                st.error(f"Erro inesperado da API: {e}")
                return None # N√£o adianta tentar de novo
    
    # Se o loop terminar, todas as tentativas falharam
    st.error(f"Falha ao gerar conte√∫do com {model.model_name} ap√≥s {max_retries} tentativas.")
    return None

# O Super-Prompt v1.1 (sem mudan√ßas)
def get_full_analysis_prompt(tema, ideia):
    return f"""
    Sua tarefa √© fazer uma an√°lise completa e contextual da 'Ideia de Post' fornecida. A sua resposta DEVE ser uma √∫nica string contendo 10 blocos. Siga as instru√ß√µes de CADA bloco com precis√£o absoluta. N√£o improvise formatos.

    **Tema Geral:** {tema}
    **Ideia de Post para An√°lise:** {ideia}

    [---CLASSIFICACAO_START---]
    (Responda APENAS com: Topo de Funil, Meio de Funil, ou Fundo de Funil)
    [---CLASSIFICACAO_END---]
    [---NOTAS_START---]
    (Avalie a ideia em 3 categorias, de 0 a 5. Use estrelas (‚òÖ) para a nota e (‚òÜ) para o que falta. Siga este formato EXATAMENTE:
    - **Criatividade:** [estrelas, ex: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ]
    - **Potencial de Viraliza√ß√£o:** [estrelas, ex: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ]
    - **Coes√£o com o Tema:** [estrelas, ex: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ])
    [---NOTAS_END---]
    [---JUSTIFICATIVA_START---]
    (Forne√ßa uma justificativa completa...)
    [---JUSTIFICATIVA_END---]
    [---FORMATO_START---]
    (Sugira o formato de post ideal... Justifique brevemente.)
    [---FORMATO_END---]
    [---FORMATO_TRANSFORM_START---]
    (Com base no formato... explique como adaptar...
    Use este formato EXATAMENTE:
    - **Para [Formato A]:** T√≠tulo: [Seu T√≠tulo para o Formato A]. Abordagem: [Sua explica√ß√£o].
    - **Para [Formato B]:** T√≠tulo: [Seu T√≠tulo para o Formato B]. Abordagem: [Sua explica√ß√£o].)
    [---FORMATO_TRANSFORM_END---]
    [---CONTEUDO_START---]
    (Sugira um t√≠tulo otimizado e crie uma lista detalhada...)
    [---CONTEUDO_END---]
    [---HASHTAGS_START---]
    (Liste EXATAMENTE 5 hashtags altamente relevantes...)
    [---HASHTAGS_END---]
    [---PROTIP_START---]
    (Ofere√ßa uma 'Dica de Mestre' estrat√©gica e elaborada...)
    [---PROTIP_END---]
    [---TRANSFORMACOES_START---]
    (Explique detalhadamente como adaptar ESTA MESMA IDEIA...)
    [---TRANSFORMACOES_END---]
    [---META_DESCRICAO_START---]
    (Este bloco √© oculto. Escreva um prompt de comando para uma IA...)
    [---META_DESCRICAO_END---]
    """

# Fun√ß√£o da "Consultoria 5 Estrelas" (sem mudan√ßas na l√≥gica do prompt)
def get_five_star_tips_prompt(tema, ideia, notas):
    return f"""
    **Contexto:** Uma ideia de post foi analisada com as seguintes notas:
    - Tema: "{tema}"
    - Ideia: "{ideia}"
    - Avalia√ß√£o: {notas}

    **Sua Tarefa:** Agir como um consultor de conte√∫do. Para CADA categoria (Criatividade, Potencial de Viraliza√ß√£o) que N√ÉO recebeu 5 estrelas, forne√ßa:
    1.  **Diagn√≥stico:** Explique brevemente (1 frase) por que a nota n√£o foi m√°xima.
    2.  **A√ß√£o Concreta:** Sugira uma mudan√ßa espec√≠fica e pr√°tica para melhorar (1 frase).

    **Ap√≥s** as an√°lises individuais, proponha UM **T√≠tulo Ideal (5 Estrelas)** que incorpore as melhorias sugeridas para maximizar Criatividade e Viraliza√ß√£o.
    **Formato da Resposta:** Use bullet points... No final, apresente o t√≠tulo com "**T√≠tulo Ideal (5 Estrelas):**".
    """

# --- INTERFACE E L√ìGICA PRINCIPAL ---

def clear_results():
    if 'show_results' in st.session_state:
        st.session_state.show_results = False
    if 'prompt_mae_gerado' in st.session_state:
        st.session_state.prompt_mae_gerado = None
    if 'five_star_tips' in st.session_state:
        st.session_state.five_star_tips = None

tema_geral_input = st.text_input("Qual o tema geral ou √°rea de atua√ß√£o?", placeholder="Constru√ß√£o Civil", on_change=clear_results)
ideia_input = st.text_input("Cole aqui o t√≠tulo ou a ideia de post que voc√™ quer analisar:", placeholder="Tend√™ncias na constru√ß√£o civil", on_change=clear_results)
model_choice = st.radio(
    "Escolha o Modelo de IA para a An√°lise:",
    ("Pro (Mais Detalhado)", "Flash (Mais R√°pido)"),
    horizontal=True,
    index=0,
    on_change=clear_results
)
button_label = "Reanalisar Ideia" if 'show_results' in st.session_state and st.session_state.show_results else "Analisar Ideia"

# NOVO: L√≥gica de Bot√£o com Redund√¢ncia
if st.button(button_label, key="analyze_button"):
    if tema_geral_input and ideia_input:
        
        # 1. Define o modelo prim√°rio e de fallback
        if "Pro" in model_choice:
            primary_model = model_pro
            fallback_model = model_flash
            st.session_state.model_in_use = model_pro # Salva para o bot√£o "5 estrelas"
        else:
            primary_model = model_flash
            fallback_model = model_pro
            st.session_state.model_in_use = model_flash
        
        st.session_state.current_tema = tema_geral_input
        st.session_state.current_ideia = ideia_input
        
        # 2. Gera o prompt
        prompt = get_full_analysis_prompt(tema_geral_input, ideia_input)

        # 3. Tenta a chamada com o modelo prim√°rio (com backoff embutido)
        with st.spinner(f"Analisando sua ideia com {primary_model.model_name}..."):
            raw_response = safe_generate_content(prompt, primary_model)
        
        # 4. Se falhar, tenta com o fallback
        if raw_response is None:
            st.warning(f"{primary_model.model_name} est√° sobrecarregado. Trocando automaticamente para {fallback_model.model_name}...")
            st.session_state.model_in_use = fallback_model # Atualiza o modelo em uso
            with st.spinner(f"Tentando com {fallback_model.model_name}..."):
                raw_response = safe_generate_content(prompt, fallback_model)
        
        # 5. Verifica se tudo falhou
        if raw_response is None:
            st.error("Falha cr√≠tica. Ambos os modelos (Pro e Flash) parecem estar indispon√≠veis ou sobrecarregados. Tente novamente mais tarde.")
            st.stop()
        
        # 6. Sucesso! Salva os resultados
        st.session_state.raw_response = raw_response
        st.session_state.show_results = True
        st.session_state.five_star_tips = None
        st.success("An√°lise conclu√≠da!")
    else:
        st.warning("Por favor, preencha ambos os campos.")

# NOVO: L√≥gica do bot√£o "5 Estrelas" tamb√©m usa o sistema de redund√¢ncia
if 'show_results' in st.session_state and st.session_state.show_results:
    # (A l√≥gica de extra√ß√£o 'r = {...}' e exibi√ß√£o do layout 'col1, col2...' continua a mesma)
    response_text = st.session_state.get('raw_response', "")

    r = {
        'classification': extrair_bloco_robusto(response_text, 'CLASSIFICACAO', 'NOTAS'),
        'notas': extrair_bloco_robusto(response_text, 'NOTAS', 'JUSTIFICATIVA'),
        'justification': extrair_bloco_robusto(response_text, 'JUSTIFICATIVA', 'FORMATO'),
        'format': extrair_bloco_robusto(response_text, 'FORMATO', 'FORMATO_TRANSFORM'),
        'format_transform': extrair_bloco_robusto(response_text, 'FORMATO_TRANSFORM', 'CONTEUDO'),
        'content': extrair_bloco_robusto(response_text, 'CONTEUDO', 'HASHTAGS'),
        'hashtags': extrair_bloco_robusto(response_text, 'HASHTAGS', 'PROTIP'),
        'pro_tip': extrair_bloco_robusto(response_text, 'PROTIP', 'TRANSFORMACOES'),
        'transformations': extrair_bloco_robusto(response_text, 'TRANSFORMACOES', 'META_DESCRICAO'),
        'meta_descricao': extrair_bloco_robusto(response_text, 'META_DESCRICAO')
    }
    
    if r.get('meta_descricao'):
        st.session_state.prompt_mae_gerado = r['meta_descricao']
    if r.get('notas'):
        st.session_state.current_notas = r['notas'] 

    st.subheader("üî¨ An√°lise Estrat√©gica da sua Ideia:")
    st.divider()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Classifica√ß√£o**"); st.metric(label="", value=r.get('classification') or "N√£o encontrado", label_visibility="collapsed")
        st.markdown("**Avalia√ß√£o da Ideia**"); st.markdown(r.get('notas') or "N√£o encontrado.") 
        
        if 'current_notas' in st.session_state:
            if st.button("Ver dicas para 5 estrelas ‚òÖ"):
                
                # Define o modelo prim√°rio e fallback para esta chamada
                primary_model = st.session_state.model_in_use
                fallback_model = model_flash if primary_model == model_pro else model_pro
                
                prompt_dicas = get_five_star_tips_prompt(
                    st.session_state.current_tema,
                    st.session_state.current_ideia,
                    st.session_state.current_notas
                )
                
                with st.spinner(f"Gerando dicas com {primary_model.model_name}..."):
                    tips = safe_generate_content(prompt_dicas, primary_model)
                
                if tips is None:
                    st.warning(f"{primary_model.model_name} est√° sobrecarregado. Trocando para {fallback_model.model_name}...")
                    with st.spinner(f"Tentando com {fallback_model.model_name}..."):
                        tips = safe_generate_content(prompt_dicas, fallback_model)

                if tips is None:
                    st.error("Falha ao gerar dicas. Tente novamente.")
                else:
                    st.session_state.five_star_tips = tips
        
        st.markdown("**Formato Ideal Sugerido**"); st.success(r.get('format') or "N√£o encontrado.", icon="üé®")
        st.markdown("**Sugest√µes de Hashtags**"); st.info(r.get('hashtags') or "N√£o encontrado.", icon="#Ô∏è‚É£")
        st.markdown("**Sugest√µes de Conte√∫do**"); st.info(r.get('content') or "N√£o encontrado.", icon="üìÑ")

    with col2:
        st.markdown("**Dica de Mestre**"); st.info(r.get('pro_tip') or "N√£o encontrado.", icon="üí°")
        st.markdown("**Justificativa e Aula**"); st.success(r.get('justification') or "N√£o encontrado.", icon="üéØ")
        
        if 'five_star_tips' in st.session_state and st.session_state.five_star_tips:
            st.markdown("**Consultoria 5 Estrelas**")
            st.info(st.session_state.five_star_tips, icon="‚ú®") 

    col3, col4 = st.columns(2)
    with col3:
        st.markdown("**Reaproveitamento (Outros Formatos)**"); st.info(r.get('format_transform') or "N√£o encontrado.", icon="‚ôªÔ∏è")
    with col4:
        st.markdown("**Reaproveitamento (Outros N√≠veis de Funil)**"); st.warning(r.get('transformations') or "N√£o encontrado.", icon="üîÑ")

    st.divider()
    if st.checkbox("Mostrar resposta bruta da IA para depura√ß√£o"):
        st.subheader("Resposta Bruta da IA")
        st.text(st.session_state.get('raw_response', 'Nenhuma resposta foi gravada.'))
